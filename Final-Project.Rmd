---
title: "Customer Behavior Analysis"
date: "Dec 6, 2022"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
```

### Team Members  
* Simin Liu  
* Iris Chen 
* Latonya Smith

## Problem Statement

Our group wants to study customer behaviors and identify customers that are more likely to take a promotion. 


Understanding customer behaviors is very important for the company, because the company can know what are the best products to promote to the customers, and what are the best channels to promote the products. If the company could identify the customers that are more likely to take a promotion, they can create some corresponding strategies that appeals to those customers, and then increase the sales. 


## Data Description
The original dataset is found on Kaggle (https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis/data). The dataset is provided by Dr. Omar Romero-Hernandez, who is a Mexican academic, engineer, and researcher currently served as a professor at U.C. Berkeley's Haas School of Business and at the Hult International Business School. 


The dataset contains 2240 obervations and 29 variables. The variables are:

* ID (num): Customer's unique identifier
* Year_Birth (num): Customer's birth year
* Education (chr, 5 levels): Customer's education level
* Marital_Status (chr, 8 levels): Customer's marital status
* Income (num): Customer's yearly household income
* Kidhome (num): Number of children in customer's household
* Teenhome (num): Number of teenagers in customer's household
* Dt_Customer (chr, date): Date of customer's enrollment with the company
* Recency (num): Number of days since customer's last purchase
* Complain (num): 1 if the customer complained in the last 2 years, 0 otherwise
* MntWines (num): Amount spent on wine in last 2 years
* MntFruits (num): Amount spent on fruits in last 2 years
* MntMeatProducts (num): Amount spent on meat in last 2 years
* MntFishProducts (num): Amount spent on fish in last 2 years
* MntSweetProducts (num): Amount spent on sweets in last 2 years
* MntGoldProds (num): Amount spent on gold in last 2 years
* NumDealsPurchases (num): Number of purchases made with a discount
* AcceptedCmp1 (num): 1 if customer accepted the offer in the 1st campaign, 0 otherwise
* AcceptedCmp2 (num): 1 if customer accepted the offer in the 2nd campaign, 0 otherwise
* AcceptedCmp3 (num): 1 if customer accepted the offer in the 3rd campaign, 0 otherwise
* AcceptedCmp4 (num): 1 if customer accepted the offer in the 4th campaign, 0 otherwise
* AcceptedCmp5 (num): 1 if customer accepted the offer in the 5th campaign, 0 otherwise
* Response (num): 1 if customer accepted the offer in the last campaign, 0 otherwise
* NumWebPurchases (num): Number of purchases made through the company’s website
* NumCatalogPurchases (num): Number of purchases made using a catalogue
* NumStorePurchases (num): Number of purchases made directly in stores
* NumWebVisitsMonth (num): Number of visits to company’s website in the last month


## Data Preprocessing

In the preliminary stages of preparing the dataset for machine learning applications, several key data manipulations were used to enhance the quality and suitability of the data for analysis. These pre-processing steps aimed to address issues related to variable transformations, missing values, and recoding. The following outlines the specific actions taken to refine the dataset:

```{r datainput, echo=TRUE}
# import libraries
library(tidyverse)
library(dplyr)
library(readr)
library(qacBase)
library(descr)
library(factoextra)
library(cluster)
library(caret)
library(qacReg)
library(qacr)

customer=read_csv("customer_personality.csv")

#show the original contents
contents(customer)

```


* Removed unuseful information:

  + Removed NAs because there were only 24 NAs in the dataset, which was a very small portion of the dataset.
  
  + Removed an outlier which refers to a customer with income more than 200000. 
  
```{r}
customer=customer %>% na.omit() %>% filter(Income<200000)
```


* Merged and recoded variables: 

  + Merged Alone, YOLO and Absurd into the Single level in Marital_Status variable. 
  
  + Recoded 2n Cycle to Technical School, Basic to High School and below, and Graduation to College Graduate in Education variable.

```{r}
customer$Marital_Status[customer$Marital_Status %in% c("Alone", "Absurd", "YOLO")]="Single"

customer$Education[customer$Education=="2n Cycle"]="Technical School"
customer$Education[customer$Education=="Basic"]="High School"
customer$Education[customer$Education=="Graduation"]="College Graduate"

```

* Creation of new variables:

  + age - created by subtracting Year_Birth from 2023 and removed an outlier whose age is more than 100.
  
  + Accepted_Cmp - created by adding AcceptedCmp1, AcceptedCmp2, AcceptedCmp3, AcceptedCmp4, AcceptedCmp5 and Response. If the sum is greater than 0, then the value of Accepted_Cmp is Yes, otherwise no. Accepted_Cmp is the final binary response variable.
  
  + children - created by adding Teenhome and Kidhome.
  
  + duration - created by calculating the difference between the current date and the date of customer's enrollment with the company.
  
  
```{r}
customer$age=2023-customer$Year_Birth
customer=customer %>% filter(age<100)

customer=customer %>% 
  mutate(Accepted_Cmp=AcceptedCmp1+AcceptedCmp2+AcceptedCmp3+
                               AcceptedCmp4+AcceptedCmp5+Response) %>% 
  mutate(Accepted_Cmp=ifelse(Accepted_Cmp>0, "Yes", "No"))

customer$children=customer$Teenhome+customer$Kidhome

customer$Dt_Customer=as.POSIXct(customer$Dt_Customer, format="%d-%m-%Y")
customer$duration=as.numeric(difftime(Sys.Date(), customer$Dt_Customer, units = "days") / 365.25)
```

* Made variables into factors: Marital_Status, Education, Accepted_Cmp, Complain

```{r}
customer=customer %>% mutate(Marital_Status=factor(Marital_Status), Education=factor(Education), Accepted_Cmp=factor(Accepted_Cmp), Complain=factor(Complain))
```

* Removed useless variables: 
```{r}
customer=customer %>% select(-c("ID", "Year_Birth", "Kidhome", "Teenhome", "Dt_Customer", "Recency", "AcceptedCmp3", "AcceptedCmp4", "AcceptedCmp5", "AcceptedCmp1", "AcceptedCmp2", "Z_CostContact", "Z_Revenue", "Response"))
```

**The dataset called customer was exported as manged_data.csv for the sake of sharing with group mates and used later for cluster analysis.**


## Machine Learning Approach

After wrapping up our data management processes, our next step involved applying various machine learning models to tackle our classification problem. More specifically, we aimed to  predict whether an individual would accept a promotion. To do this, we designated "Accepted_Cmp" as our response variable (what we want to predict). As it is a binary response variable, for every model, we used metric=“ROC”, family=“binomial”, and set the trControl = trainControl(method = "cv", number = 10, summaryFunction = twoClassSummary, classProbs = TRUE) and we fed the remaining variables as predictors.


- **KNN**
K-Nearest Neighbors (KNN) is a straightforward algorithm that can be used for classification and regression. The underlying principle is based on the idea that similar things are close to each other. In the context of classification, when predicting the class of a new data point, KNN examines the classes of its k nearest neighbors in the feature space by calculating Euclidean distances. The class that occurs most frequently among these neighbors is assigned to the new data point. KNN's performance can be sensitive to the choice of the distance metric and the value of k. For our knn model, we tried 10 different k values. As a result, the model chose 5 as the best nearest neighbor value, with a sensitivity of 0.9013142, a specificity of 0.3657738 and a ROC of 0.7015743.

- **Logistic Regression**
Logistic Regression is a fundamental and widely used linear model for binary classification tasks. It's particularly well-suited when the relationship between the features and the binary outcome can be approximated by a linear function. Logistic Regression models the probability of an instance belonging to a certain class using the logistic function, which ensures that the predicted probabilities fall between 0 and 1. The model estimates coefficients for each feature, indicating the strength and direction of their influence on the target variable. Despite its simplicity, Logistic Regression often performs well and is interpretable, making it a popular choice for various applications that prioritize interpretability. Our lr model yielded a sensitivity of 0.9145773, a specificity of 0.3903061, and a ROC of 0.7676095 on the training data.


- **Logistic Regression with Backward Stepwise Selection**
Logistic Regression with Backward Stepwise Selection enhances the traditional logistic regression by automating the feature selection process. Starting with a model that includes all available features, this method iteratively removes the least significant features until a stopping criterion is met (i.e. taking out a variable that hurts the overall equation). The goal is to simplify the model by eliminating irrelevant or redundant variables, thereby improving interpretability and potentially preventing overfitting. Backward Stepwise Selection is particularly useful when dealing with datasets with a large number of features. For our model, it ended up with 11 variables (Marital_StatusSingle, Income, MntWines, MntFruits, MntMeatProducts, MntGoldProds, NumDealsPurchases, NumCatalogPurchases, NumStorePurchases, NumWebVisitsMonth, children) and an AIC value of 1714.55.

- **Logistic Regression with Regularization**
Logistic Regression with Regularization is a variant that addresses the risk of overfitting by penalizing large coefficients. Regularization techniques, such as Lasso Regression and Ridge Regression, add a penalty term to the logistic regression cost function, discouraging the model from assigning excessive importance to any single feature. This is especially valuable when dealing with high-dimensional data where many features may not contribute significantly to the model's predictive power. Regularized Logistic Regression strikes a balance between fitting the training data well and generalizing to new, unseen data, making it a robust choice for various classification problems. Our model chose an alpha of 0.7 which is the relative weight of the ridge and lasso penalties, while a lambda of 0.005533647 determines the overall strength of the regularizations.

- **Classification Tree**
A Classification Tree is a hierarchical structure that makes decisions based on the values of features. At each internal node, the tree evaluates a specific feature, splitting the data into subsets based on the feature's value. This process is repeated recursively until a stopping criterion, such as a maximum depth or minimum number of samples per leaf, is reached. Classification Trees are interpretable and easy to visualize, providing a clear decision-making path. However, they can be prone to overfitting, having a bad performance on new data. Our model chose a complexity parameter(cp) of 0.003615702, which added penalty to the tree for having too many splits. The smaller the cp, the larger the tree and vice versa. As a result, the sensitivity, specificity and ROC are 0.8849867, 0.4649660 and 0.7666539, respectively. 

- **Random Forest**
One of the pruning techniques that enhances the predictive performance of individual tree is Random Forest. It is an ensemble learning method that builds multiple decision trees and combines their predictions to achieve higher accuracy and robustness. Each tree is constructed using a random subset of the training data and a random subset of features at each split. The final prediction is often an average or majority vote of the individual tree predictions, providing a more stable and accurate result. Random Forest is known for handling complex relationships within the data and is less prone to overfitting compared to individual decision trees. Our model built 500 trees and ended up choosing 4 features randomly at each split. As a result, the model yielded a sensitivity of 0.9308321, a specificity of 0.5040391, and a ROC of 0.8480844 on the training data.

- **Gradient Boost Machine**
The Gradient Boost Machine is another powerful ensemble learning technique that builds a series of weak learners, typically decision trees, sequentially. Each tree corrects the errors made by the previous ones, focusing on the instances that were misclassified. By combining the predictions of multiple weak learners, the model creates a strong predictive ensemble. Gradient Boosting is adept at capturing complex patterns and interactions within the data, making it particularly effective in high-dimensional spaces. However, it requires careful tuning of hyperparameters and may be computationally more demanding than some other algorithms. Its robustness and predictive performance have made Gradient Boosting a popular choice in various real-world applications. For our model, we tried 10 different values for each of the 4 following hyperparameters: n.trees (number of trees), interaction.depth(maximum nodes per tree), shrinkage(learning rate), and n.minobsinnode(minimum number of observations in terminal nodes). The final model has n.trees=150, interaction.depth=6, shrinkage=0.1, and n.minobsinnode=10. The ROC value is 0.8393664, the specificity is 0.5120748, and the sensitivity is 0.9091146.


```{r models, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
# create train(80%) and test dataset(20%)
library(caret)
set.seed(1234)
index = createDataPartition(customer$Accepted_Cmp, p=.8, list=FALSE)
train = customer[index,]
test = customer[-index,]

# use 10-fold cross-validation
tc = trainControl(method = "cv", number = 10,
                  summaryFunction = twoClassSummary,
                  classProbs = TRUE)

# try 10 values for each hyperparameter
# KNN 
set.seed(1234)
model.knn <- train(Accepted_Cmp ~ . , 
                   data = train, 
                   method = "knn",
                   metric = "ROC",
                   trControl = tc, 
                   tuneLength = 10)

# Logistic Regression
set.seed(1234)
model.lr <- train(Accepted_Cmp ~ .,
                  data = train,
                  method = "glm",
                  family = "binomial",
                  metric = "ROC",
                  trControl = tc)

# Logistic Regression with Backward Stepwise Selection
set.seed(1234)
model.lrsw <- train(Accepted_Cmp ~ .,
                    data = train,
                    method = "glmStepAIC",
                    family = "binomial",
                    metric = "ROC",
                    trControl = tc)

# Logistic Regression with Regularization
set.seed(1234)
model.lrreg <- train(Accepted_Cmp ~ .,
                     data = train,
                     method = "glmnet",
                     family = "binomial",
                     metric = "ROC",
                     tuneLength = 10,
                     trControl = tc)

# Classification Tree
set.seed(1234)
model.ctree <- train(Accepted_Cmp ~., 
                     data = train, 
                     method = "rpart",
                     trControl=tc,
                     metric = "ROC",
                     tuneLength = 10)

# Random Forest
set.seed(1234)
model.rf <- train(Accepted_Cmp ~ ., 
                  data = train, 
                  method = "rf",
                  metric = "ROC",
                  tuneLength = 10,
                  varImp=TRUE,
                  trControl=tc)

# Gradient Boost Machine
set.seed(1234)
model.gbm <- train(Accepted_Cmp~., 
                   data = train, 
                   method = "gbm",
                   metric = "ROC",
                   tuneLength=10,
                   verbose = FALSE,
                   trControl=tc)
```

## Results

```{r analysis}
# compare models
results <- resamples(list(knn = model.knn, 
                          lr = model.lr, 
                          lrsw = model.lrsw, 
                          lrreg = model.lrreg,
                          rf = model.rf,
                          ctree = model.ctree,
                          gbm = model.gbm))
bwplot(results)

## Examine the ROC curve (choose 0.19 as final cutoff point)
library(qacReg)
prob <- predict(model.rf, train, type="prob")[[2]]
roc_plot(train$Accepted_Cmp, prob)

# evaluate on test data
test$Accepted_Cmp <- factor(test$Accepted_Cmp)
prob <- predict(model.rf, test, type = "prob")[[2]]
pred <- ifelse(prob > 0.19, 1, 0)
pred <- factor(pred, levels = c(0, 1), labels = c("No", "Yes"))
confusionMatrix(pred, test$Accepted_Cmp, positive = "Yes")

# variable importance
plot(varImp(model.rf))
```

Among all machine learning models, Random Forest performs the best. The Random Forest model predicts around 89% of customers who take promotion will say "Yes" and around 64% of customers who do not take promotion will say "No". There is around 70% probability that the model predicts accurately. Amount spent on wines in the last 2 years has the greatest influence on predicting whether customers will take promotion or not, followed by their income and amount spent on meat in the last 2 years.

```{r}
# Add columns of prob and pred
test$prob = prob
test$pred = pred
```

**test was exported as test_data.csv for the sake of sharing with group mates and was imported later for cluster analysis.**


## Discussion

In order to answer the question the research question, it was necessary to do additional analysis. A cluster analysis was done on the original observed data to have preliminary results showcasing the groups of individuals who would accept a promotion campaign. In order to do this, a subset of those who accepted the promotion was extracted and the clustering was done to identify distinct groups within this subset. The results of this is shown below.

```{r, include=TRUE, echo=TRUE}
#managed_data.csv is the customer dataset after data management. The dataset called customer as exported as manged_data.csv for the sake of sharing with group mates and imported here.

managed_data<- read_csv("managed_data.csv")

managed_data_promotionsY<- managed_data %>%
 filter(Accepted_Cmp == "Yes") 


## 
data_vbls<- managed_data_promotionsY [c("Income", "MntWines", "MntFruits", 
                                    "MntMeatProducts","MntFishProducts",
                                    "MntSweetProducts", "MntGoldProds", 
                                    "duration", "age", "children")]
data_vbls<- scale(data_vbls)
fviz_nbclust(data_vbls, kmeans, method = "wss") #within sum squares
#By the elbow graph, 3 is the turning point, so we choose to build 3 clusters.
cust_type1 = kmeans(data_vbls, centers = 3, nstart = 100)
managed_data_promotionsY$class<- cust_type1$cluster
data_vbls=as.data.frame(data_vbls)
x=cust_type1$cluster
data_vbls$type=x
```


```{r, echo=TRUE}
profile_plot(data_vbls, cluster="type", type="bar")+
 scale_fill_brewer(palette = "Set3")
```

From the above plot, we can see that our preliminary cluster analysis identified three different customer types. One customer type is identified as people who had a medium level income compared to the other groups, and who spent most of their grocery allotted money on wine. Another customer type showcased individuals who spent most of their money on a variety of food items such as fish, fruits, meat, sweets and wine. This customer type had the highest income compared to the other groups. Lastly, this analysis identified a customer type of which individuals had more children compared to the other groups.

Once our Random Forest model was created, we used it to predict individuals who would take a campaign promotion on our test dataset.  The results of our model prediction was used for another cluster analysis, using the same method described above. This allowed us to illustrate the accuracy of our model, because a comparison between the two cluster results would ascertain whether our model predicted the correct customer types.

```{r, include=TRUE, echo=TRUE}
#test_data is the same as the testing dataset. It was exported as a csv file and imported here. test_data.csv is the test dataset after partition and prediction.

test_data<- read_csv("test_data.csv")
test_data_promotionsY<- test_data %>%
 filter(pred == "Yes") 

 
#Creating the clusters
data_variables<- test_data_promotionsY [c("Income", "MntWines", "MntFruits", 
                           "MntMeatProducts", "MntFishProducts", 
                           "MntSweetProducts", "MntGoldProds",
                           "duration","age","children")]
data_variables=scale(data_variables)
fviz_nbclust(data_variables, kmeans, method = "wss") #within sum squares
cust_type = kmeans(data_variables, centers = 3, nstart = 100)
test_data_promotionsY$class<- cust_type$cluster
data_variables=as.data.frame(data_variables)
x=cust_type$cluster
data_variables$type=x

#Visualizing the created clusters
```


```{r plot, echo=TRUE}
profile_plot(data_variables, cluster="type", type="bar")+
 scale_fill_brewer(palette = "Set3")
```

The above cluster plot is almost identical to the previous one done on the originally observed data. It identifies three different customer types. One who spends most of their grocery allotted money on a variety of grocery items consisting of fish, fruits meat and sweet products, wine and has a high income. Another type was identified as customers who have a medium level income compared to its counter parts who spent the most amount of money on wine. The other customer type is one where they have more children compared to the others. 

There are only slight differences in the height of each cluster input, and therefore intensity of their attribution to the customer type. This can be seen when columns such as MntWines and MntSweetProducts are examined. However, the customer types are the same. Therefore we can confirm that our model was accurate and answered our question well.

The implications of this model are that it can now accurately predict the type of customers that would accept a promotion. This would improve sales as there is now a profile of who to target when creating them. As a result these promotions may be tailored directly for maximum profit. It may also provide insight into what items are best to be used in promotion campaigns as certain customer types would be more inclined to purchase specific products.

In order to create an even more robust model, it would be beneficial to use a larger dataset, ideally one that has a greater number of individuals who said yes to the promotion. This would have provided us with more information that would increase the accuracy of the model and as a result give a more robust profile for each customer type. 

It would also be ideal to expound on some variables that are already in the dataset. For example, it would be a good idea to have some information on the promotion campaigns such as the products they are centered around along with their prices. We could also add other variables that would better predict whether or not customers would accept a promotion. For example, demographic information about the customers would provide a deeper layer of insight into their motivations behind accepting a promotion or not, such as their gender and ethnic background. This could lay the foundation for future insight through a Latent Variable analysis.These categorical variables could help to identify customer types as well.

To conclude, we found that Random Forest was the best model to be used to create a predictor of customer type for a grocery store.


## References
Patel, A. (2021, August 22). Customer personality analysis. Kaggle. https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis/data